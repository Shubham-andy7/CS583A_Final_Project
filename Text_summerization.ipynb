{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPfc7b32w4dZkOoIDZkXwZG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"557fFCQAG1sA","executionInfo":{"status":"ok","timestamp":1700525300320,"user_tz":300,"elapsed":6696,"user":{"displayName":"aman gupta","userId":"17323949785260642657"}}},"outputs":[],"source":["import re\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed\n","from keras.callbacks import EarlyStopping\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"code","source":["MAX_TEXT_LEN = 100\n","MAX_SUMMARY_LEN = 15\n","LATENT_DIM = 300\n","EMBEDDING_DIM = 200"],"metadata":{"id":"eFFRCAMrKbYP","executionInfo":{"status":"ok","timestamp":1700525319692,"user_tz":300,"elapsed":197,"user":{"displayName":"aman gupta","userId":"17323949785260642657"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Data preprocessing function\n","def preprocess_data():\n","    # Load data\n","    summary = pd.read_csv('/content/news_summary.csv', encoding='iso-8859-1')\n","    raw = pd.read_csv('/content/news_summary_more.csv', encoding='iso-8859-1')\n","\n","    # Data cleaning and processing\n","    pre1 = raw.iloc[:, 0:2].copy()\n","    pre2 = summary.iloc[:, 0:6].copy()\n","    pre = pd.DataFrame()\n","    pre['text'] = pd.concat([pre1['text'], pre2['text']], ignore_index=True)\n","    pre['summary'] = pd.concat([pre1['headlines'], pre2['headlines']], ignore_index=True)\n","\n","    # Clean text using NLTK\n","    pre['cleaned_text'] = pre['text'].apply(lambda x: ' '.join(word_tokenize(x)))\n","    pre['cleaned_summary'] = pre['summary'].apply(lambda x: ' '.join(word_tokenize(x)))\n","\n","    # Tokenize text and summary\n","    x_tokenizer = Tokenizer()\n","    x_tokenizer.fit_on_texts(list(pre['cleaned_text']))\n","    y_tokenizer = Tokenizer()\n","    y_tokenizer.fit_on_texts(list(pre['cleaned_summary']))\n","\n","    # Pad sequences\n","    x_tr_seq = x_tokenizer.texts_to_sequences(x_tr)\n","    x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n","    y_tr_seq = y_tokenizer.texts_to_sequences(y_tr)\n","    y_val_seq = y_tokenizer.texts_to_sequences(y_val)\n","\n","    x_tr = pad_sequences(x_tr_seq, maxlen=MAX_TEXT_LEN, padding='post')\n","    x_val = pad_sequences(x_val_seq, maxlen=MAX_TEXT_LEN, padding='post')\n","    y_tr = pad_sequences(y_tr_seq, maxlen=MAX_SUMMARY_LEN, padding='post')\n","    y_val = pad_sequences(y_val_seq, maxlen=MAX_SUMMARY_LEN, padding='post')\n","\n","    # Split data\n","    x_tr, x_val, y_tr, y_val = train_test_split(x_tr, y_tr, test_size=0.1, random_state=42, shuffle=True)\n","\n","    return x_tr, x_val, y_tr, y_val, x_tokenizer, y_tokenizer\n"],"metadata":{"id":"O5Id9p7sKba7","executionInfo":{"status":"ok","timestamp":1700525365799,"user_tz":300,"elapsed":166,"user":{"displayName":"aman gupta","userId":"17323949785260642657"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKEbrqC3NDxm","executionInfo":{"status":"ok","timestamp":1700525368940,"user_tz":300,"elapsed":658,"user":{"displayName":"aman gupta","userId":"17323949785260642657"}},"outputId":"88c94e49-795d-405b-c7a9-835f308433da"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["def build_model(x_voc, y_voc):\n","    # Encoder\n","    encoder_inputs = Input(shape=(MAX_TEXT_LEN,))\n","    enc_emb = Embedding(x_voc, EMBEDDING_DIM, trainable=True)(encoder_inputs)\n","    # ...\n","\n","    # Decoder\n","    decoder_inputs = Input(shape=(None,))\n","    dec_emb_layer = Embedding(y_voc, EMBEDDING_DIM, trainable=True)\n","    dec_emb = dec_emb_layer(decoder_inputs)\n","    # ...\n","\n","    # Compile the model\n","    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","\n","    return model\n"],"metadata":{"id":"zjpRW7sSKbd-","executionInfo":{"status":"ok","timestamp":1700525370793,"user_tz":300,"elapsed":188,"user":{"displayName":"aman gupta","userId":"17323949785260642657"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def train_model(model, x_tr, y_tr, x_val, y_val):\n","    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n","\n","    history = model.fit([x_tr, y_tr[:, :-1]], y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n","                        epochs=50, callbacks=[es], batch_size=128,\n","                        validation_data=([x_val, y_val[:, :-1]], y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:, 1:]))\n","\n","    return history\n","\n"],"metadata":{"id":"5PkO430hKbgp","executionInfo":{"status":"ok","timestamp":1700525373013,"user_tz":300,"elapsed":149,"user":{"displayName":"aman gupta","userId":"17323949785260642657"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Inference function\n","def generate_summary(model, input_seq, x_tokenizer, y_tokenizer, max_summary_len):\n","    # Encode the input sequence to get the feature vector\n","    encoder_model = Model(inputs=model.input[0], outputs=model.get_layer('encoder_lstm_3').output[0])\n","\n","    # Decoder setup\n","    decoder_state_input_h = Input(shape=(LATENT_DIM,))\n","    decoder_state_input_c = Input(shape=(LATENT_DIM,))\n","    decoder_hidden_state_input = Input(shape=(MAX_TEXT_LEN, LATENT_DIM))\n","\n","    dec_emb_layer = model.get_layer('embedding_2')\n","    dec_emb2 = dec_emb_layer(model.input[1])\n","\n","    decoder_lstm = model.get_layer('lstm_2')\n","    decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n","\n","    decoder_dense = model.get_layer('time_distributed_1')\n","    decoder_outputs2 = decoder_dense(decoder_outputs2)\n","\n","    decoder_model = Model(\n","        [model.input[1]] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n","        [decoder_outputs2] + [state_h2, state_c2])\n","\n","    # Inference\n","    target_seq = np.zeros((1, 1))\n","    target_seq[0, 0] = y_tokenizer.word_index['sostok']\n","\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict([target_seq] + [encoder_model.predict(input_seq)] + [np.zeros((1, MAX_TEXT_LEN, LATENT_DIM)),\n","                                                                                                       np.zeros((1, LATENT_DIM)),\n","                                                                                                       np.zeros((1, LATENT_DIM))])\n","\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_token = reverse_target_word_index[sampled_token_index]\n","\n","        if sampled_token != 'eostok':\n","            decoded_sentence += ' ' + sampled_token\n","\n","        if sampled_token == 'eostok' or len(decoded_sentence.split()) >= (max_summary_len - 1):\n","            stop_condition = True\n","\n","        target_seq = np.zeros((1, 1))\n","        target_seq[0, 0] = sampled_token_index\n","\n","    return decoded_sentence\n"],"metadata":{"id":"1hBsM4DlKbjS","executionInfo":{"status":"ok","timestamp":1700525374818,"user_tz":300,"elapsed":144,"user":{"displayName":"aman gupta","userId":"17323949785260642657"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Main code\n","if __name__ == \"__main__\":\n","    x_tr, x_val, y_tr, y_val, x_tokenizer, y_tokenizer = preprocess_data()\n","\n","    # Build and compile the model\n","    model = build_model(x_tokenizer.num_words + 1, y_tokenizer.num_words + 1)\n","\n","    # Train the model\n","    history = train_model(model, x_tr, y_tr, x_val, y_val)\n","\n","    # Sample inference\n","    for i in range(5):\n","        print(\"Review:\", seq2text(x_tr[i], x_tokenizer))\n","        print(\"Original summary:\", seq2summary(y_tr[i], y_tokenizer))\n","        print(\"Predicted summary:\", generate_summary(model, x_tr[i].reshape(1, MAX_TEXT_LEN), x_tokenizer, y_tokenizer))\n","        print(\"\\n\")"],"metadata":{"id":"qChiBxEUKbl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uLoPVwQSKbo3"},"execution_count":null,"outputs":[]}]}